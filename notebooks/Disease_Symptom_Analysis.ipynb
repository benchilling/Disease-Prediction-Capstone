{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Symptom Analysis and Prediction\n",
    "## Capstone Project for Healthcare Insights\n",
    "\n",
    "### Introduction\n",
    "This notebook analyzes the \"Disease Symptom Description Dataset\" from Kaggle to build a predictive model that can identify diseases based on patient symptoms. Early and accurate disease identification is crucial for timely treatment and better patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For model building and evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For data visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "# Note: Adjust file paths as needed based on your environment\n",
    "dataset_path = 'dataset.csv'  # Main dataset with diseases and symptoms\n",
    "severity_path = 'Symptom-severity.csv'  # Symptom severity information\n",
    "description_path = 'symptom_Description.csv'  # Disease descriptions\n",
    "precaution_path = 'symptom_precaution.csv'  # Precautions for diseases\n",
    "\n",
    "# Load the main dataset with diseases and symptoms\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(\"Main dataset loaded successfully\")\n",
    "    print(f\"Shape of the dataset: {df.shape}\")\n",
    "    df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find the dataset file. Please check the file path.\")\n",
    "    \n",
    "# Load the symptom severity information\n",
    "try:\n",
    "    severity_df = pd.read_csv(severity_path)\n",
    "    print(\"Symptom severity dataset loaded successfully\")\n",
    "    print(f\"Shape of the symptom severity dataset: {severity_df.shape}\")\n",
    "    severity_df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find the symptom severity file. Please check the file path.\")\n",
    "    \n",
    "# Load the disease descriptions\n",
    "try:\n",
    "    description_df = pd.read_csv(description_path)\n",
    "    print(\"Disease description dataset loaded successfully\")\n",
    "    print(f\"Shape of the disease description dataset: {description_df.shape}\")\n",
    "    description_df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find the disease description file. Please check the file path.\")\n",
    "    \n",
    "# Load the disease precautions\n",
    "try:\n",
    "    precaution_df = pd.read_csv(precaution_path)\n",
    "    print(\"Disease precaution dataset loaded successfully\")\n",
    "    print(f\"Shape of the disease precaution dataset: {precaution_df.shape}\")\n",
    "    precaution_df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find the disease precaution file. Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the main dataset\n",
    "print(\"Basic information about the main dataset:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Count unique diseases and symptoms\n",
    "disease_counts = df['Disease'].value_counts()\n",
    "num_unique_diseases = len(disease_counts)\n",
    "print(f\"\\nNumber of unique diseases: {num_unique_diseases}\")\n",
    "\n",
    "# Get all symptom columns\n",
    "symptom_cols = [col for col in df.columns if 'Symptom' in col]\n",
    "\n",
    "# Gather all symptoms\n",
    "all_symptoms = []\n",
    "for col in symptom_cols:\n",
    "    all_symptoms.extend(df[col].dropna().values)\n",
    "\n",
    "# Count unique symptoms\n",
    "unique_symptoms = set(all_symptoms)\n",
    "print(f\"Number of unique symptoms: {len(unique_symptoms)}\")\n",
    "\n",
    "# Display the top 10 most common diseases\n",
    "print(\"\\nTop 10 most common diseases:\")\n",
    "print(disease_counts.head(10))\n",
    "\n",
    "# Display the top 10 most common symptoms\n",
    "symptom_counter = Counter(all_symptoms)\n",
    "top_symptoms = symptom_counter.most_common(10)\n",
    "print(\"\\nTop 10 most common symptoms:\")\n",
    "for symptom, count in top_symptoms:\n",
    "    print(f\"{symptom}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of diseases\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(y='Disease', data=df, order=df['Disease'].value_counts().index[:15])\n",
    "plt.title('Top 15 Most Common Diseases')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Disease')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of symptoms\n",
    "top_20_symptoms = dict(symptom_counter.most_common(20))\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.bar(top_20_symptoms.keys(), top_20_symptoms.values())\n",
    "plt.title('Top 20 Most Common Symptoms')\n",
    "plt.xlabel('Symptom')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check symptom severity distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(severity_df['weight'], bins=7, kde=True)\n",
    "plt.title('Distribution of Symptom Severity Weights')\n",
    "plt.xlabel('Severity Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between symptoms and diseases (heatmap for top diseases and symptoms)\n",
    "top_diseases = disease_counts.head(10).index\n",
    "top_symptoms_names = [symptom for symptom, _ in top_symptoms]\n",
    "\n",
    "# Create a matrix to count symptoms for each disease\n",
    "heatmap_data = np.zeros((len(top_diseases), len(top_symptoms_names)))\n",
    "\n",
    "for i, disease in enumerate(top_diseases):\n",
    "    disease_rows = df[df['Disease'] == disease]\n",
    "    for j, symptom in enumerate(top_symptoms_names):\n",
    "        count = 0\n",
    "        for col in symptom_cols:\n",
    "            count += (disease_rows[col] == symptom).sum()\n",
    "        heatmap_data[i, j] = count\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='g', cmap='viridis', \n",
    "            xticklabels=top_symptoms_names, yticklabels=top_diseases)\n",
    "plt.title('Relationship Between Top Diseases and Symptoms')\n",
    "plt.xlabel('Symptoms')\n",
    "plt.ylabel('Diseases')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps\n",
    "def preprocess_data(df, severity_df):\n",
    "    # Create a copy of the dataframe\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Fill missing values in symptom columns with '0'\n",
    "    symptom_cols = [col for col in processed_df.columns if 'Symptom' in col]\n",
    "    processed_df[symptom_cols] = processed_df[symptom_cols].fillna('0')\n",
    "    \n",
    "    # Create a symptom severity dictionary for quick lookup\n",
    "    severity_dict = dict(zip(severity_df['Symptom'], severity_df['weight']))\n",
    "    # Default value for missing symptoms\n",
    "    default_severity = 0\n",
    "    \n",
    "    # Replace symptom names with their severity weights\n",
    "    for col in symptom_cols:\n",
    "        processed_df[col] = processed_df[col].map(lambda x: severity_dict.get(x, default_severity))\n",
    "    \n",
    "    # Encode the target variable (Disease)\n",
    "    le = LabelEncoder()\n",
    "    processed_df['Disease'] = le.fit_transform(processed_df['Disease'])\n",
    "    \n",
    "    # Save the encoder classes for later reference\n",
    "    disease_classes = le.classes_\n",
    "    \n",
    "    return processed_df, disease_classes, le\n",
    "\n",
    "# Preprocess the data\n",
    "processed_df, disease_classes, label_encoder = preprocess_data(df, severity_df)\n",
    "\n",
    "# Split features and target\n",
    "X = processed_df.drop('Disease', axis=1)\n",
    "y = processed_df['Disease']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                          cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the model with best hyperparameters\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = best_rf_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Other evaluation metrics\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(best_rf_model, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=disease_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='g', cmap='Blues', \n",
    "            xticklabels=disease_classes, yticklabels=disease_classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(15))\n",
    "plt.title('Top 15 Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business and Health Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract business and health insights\n",
    "\n",
    "# 1. Identify the most predictive symptoms\n",
    "print(\"Most predictive symptoms based on feature importance:\")\n",
    "for feature, importance in zip(feature_imp_df['Feature'].head(5), feature_imp_df['Importance'].head(5)):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate accuracy for each disease to identify diseases that are easier or harder to predict\n",
    "disease_accuracy = {}\n",
    "for disease in np.unique(y_test):\n",
    "    disease_indices = np.where(y_test == disease)[0]\n",
    "    disease_predictions = y_test_pred[disease_indices]\n",
    "    disease_accuracy[disease] = accuracy_score(np.full_like(disease_predictions, disease), disease_predictions)\n",
    "\n",
    "# Convert disease indices back to names\n",
    "disease_accuracy_named = {disease_classes[disease]: acc for disease, acc in disease_accuracy.items()}\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_disease_accuracy = sorted(disease_accuracy_named.items(), key=lambda x: x[1])\n",
    "\n",
    "# Diseases that are difficult to predict (lowest accuracy)\n",
    "print(\"\\nDiseases that are most difficult to predict accurately:\")\n",
    "for disease, acc in sorted_disease_accuracy[:5]:\n",
    "    print(f\"{disease}: {acc:.4f}\")\n",
    "\n",
    "# Diseases that are easy to predict (highest accuracy)\n",
    "print(\"\\nDiseases that are easiest to predict accurately:\")\n",
    "for disease, acc in sorted_disease_accuracy[-5:]:\n",
    "    print(f\"{disease}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analyze symptom co-occurrence\n",
    "symptom_cols = [col for col in df.columns if 'Symptom' in col]\n",
    "symptom_pairs = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    symptoms = [symptom for symptom in row[symptom_cols].values if isinstance(symptom, str) and symptom != '0']\n",
    "    for i in range(len(symptoms)):\n",
    "        for j in range(i + 1, len(symptoms)):\n",
    "            pair = tuple(sorted([symptoms[i], symptoms[j]]))\n",
    "            if pair in symptom_pairs:\n",
    "                symptom_pairs[pair] += 1\n",
    "            else:\n",
    "                symptom_pairs[pair] = 1\n",
    "\n",
    "# Get top co-occurring symptoms\n",
    "top_pairs = sorted(symptom_pairs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop co-occurring symptom pairs:\")\n",
    "for (symptom1, symptom2), count in top_pairs:\n",
    "    print(f\"{symptom1} & {symptom2}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analyze disease severity by calculating average symptom severity for each disease\n",
    "disease_severity = {}\n",
    "for disease in df['Disease'].unique():\n",
    "    disease_data = df[df['Disease'] == disease]\n",
    "    symptoms = []\n",
    "    for col in symptom_cols:\n",
    "        symptoms.extend([s for s in disease_data[col].values if isinstance(s, str) and s != '0'])\n",
    "    \n",
    "    # Get severity for each symptom\n",
    "    severity_dict = dict(zip(severity_df['Symptom'], severity_df['weight']))\n",
    "    severities = [severity_dict.get(s, 0) for s in symptoms]\n",
    "    avg_severity = np.mean(severities) if severities else 0\n",
    "    disease_severity[disease] = avg_severity\n",
    "\n",
    "# Sort by severity\n",
    "sorted_severity = sorted(disease_severity.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Highest severity diseases\n",
    "print(\"\\nDiseases with highest symptom severity:\")\n",
    "for disease, severity in sorted_severity[:5]:\n",
    "    print(f\"{disease}: {severity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Healthcare resource allocation insights\n",
    "print(\"\\nInsights for healthcare resource allocation:\")\n",
    "print(\"Based on disease prevalence and difficulty of diagnosis, the following areas may need more resources:\")\n",
    "for disease, count in disease_counts.head(5).items():\n",
    "    acc = disease_accuracy_named.get(disease, 0)\n",
    "    print(f\"{disease}: High prevalence ({count} cases), Diagnostic accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing with Custom Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict disease based on symptoms\n",
    "def predict_disease(symptoms, rf_model, le, severity_df):\n",
    "    # Create a symptom severity dictionary for quick lookup\n",
    "    severity_dict = dict(zip(severity_df['Symptom'], severity_df['weight']))\n",
    "    # Default value for missing symptoms\n",
    "    default_severity = 0\n",
    "    \n",
    "    # Get all symptom columns from model features\n",
    "    symptom_cols = [col for col in X.columns]\n",
    "    \n",
    "    # Initialize an empty array for input features\n",
    "    input_data = np.zeros(len(symptom_cols))\n",
    "    \n",
    "    # Map input symptoms to their severity\n",
    "    for i, symptom in enumerate(symptoms):\n",
    "        if i < len(symptom_cols):  # Ensure we don't exceed the number of symptom columns\n",
    "            input_data[i] = severity_dict.get(symptom, 0)\n",
    "    \n",
    "    # Reshape the data for prediction\n",
    "    input_data = input_data.reshape(1, -1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = rf_model.predict(input_data)\n",
    "    \n",
    "    # Convert prediction index to disease name\n",
    "    disease = le.inverse_transform(prediction)[0]\n",
    "    \n",
    "    # Get probability scores\n",
    "    proba = rf_model.predict_proba(input_data)[0]\n",
    "    top_indices = np.argsort(proba)[::-1][:3]  # Get indices of top 3 diseases\n",
    "    top_diseases = [(le.inverse_transform([idx])[0], proba[idx]) for idx in top_indices]\n",
    "    \n",
    "    return disease, top_diseases\n",
    "\n",
    "# Example test case\n",
    "test_symptoms = ['fatigue', 'high_fever', 'headache', 'cough']\n",
    "predicted_disease, top_3_diseases = predict_disease(test_symptoms, best_rf_model, label_encoder, severity_df)\n",
    "\n",
    "print(f\"Predicted disease: {predicted_disease}\")\n",
    "print(\"Top 3 diseases with probabilities:\")\n",
    "for disease, probability in top_3_diseases:\n",
    "    print(f\"{disease}: {probability:.4f}\")\n",
    "\n",
    "# Get disease description and precautions if available\n",
    "try:\n",
    "    description = description_df[description_df['Disease'] == predicted_disease]['Description'].values[0]\n",
    "    print(f\"\\nDisease Description: {description}\")\n",
    "except (IndexError, KeyError):\n",
    "    print(\"\\nDescription not available for this disease.\")\n",
    "\n",
    "try:\n",
    "    precautions = precaution_df[precaution_df['Disease'] == predicted_disease]\n",
    "    if not precautions.empty:\n",
    "        print(\"\\nPrecautions:\")\n",
    "        for i in range(1, 5):  # There are 4 precaution columns\n",
    "            precaution = precautions[f'Precaution_{i}'].values[0]\n",
    "            if precaution and isinstance(precaution, str):\n",
    "                print(f\"{i}. {precaution}\")\n",
    "except (IndexError, KeyError):\n",
    "    print(\"\\nPrecautions not available for this disease.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Performance:\n",
    "\n",
    "The Random Forest model achieved an accuracy of 0.9240 on the test set.\n",
    "Mean cross-validation score: 0.9182\n",
    "The model shows good generalization with consistent performance across validation folds.\n",
    "\n",
    "2. Key Business and Health Insights:\n",
    "\n",
    "Early identification of high-risk diseases should be prioritized in healthcare systems.\n",
    "Symptom severity can help in triaging patients more effectively.\n",
    "The most predictive symptoms should be included in initial screening protocols.\n",
    "Diseases with lower predictive accuracy may require additional diagnostic tests.\n",
    "\n",
    "3. Future Improvements:\n",
    "\n",
    "Include demographic information to improve prediction accuracy.\n",
    "Incorporate time-based symptom progression for better diagnosis.\n",
    "Develop a user-friendly interface for healthcare professionals.\n",
    "Add confidence intervals to predictions for more informed decision-making.\n",
    "Explore deployment options for integration with existing healthcare systems.\n",
    "\n",
    "4. Healthcare System Impact:\n",
    "\n",
    "Implementation could reduce diagnostic time by providing early disease candidates.\n",
    "Can serve as an initial screening tool to prioritize urgent cases.\n",
    "May improve resource allocation in understaffed healthcare facilities.\n",
    "Could help reduce misdiagnoses by providing a data-driven second opinion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
